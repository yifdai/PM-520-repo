{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yifdai/PM-520-repo/blob/main/HW/PM520_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2. Maximum likelihood & Optimization Crash Course"
      ],
      "metadata": {
        "id": "SB19uPlEpw4u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0RgUYaXfIWf"
      },
      "outputs": [],
      "source": [
        "!pip install lineax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.numpy.linalg as jnpla\n",
        "import jax.scipy as jsp\n",
        "import jax.scipy.linalg as jspla"
      ],
      "metadata": {
        "id": "0aNnRcuZfTXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Ordinary least squares (i.e. OLS)\n",
        "OLS is an approach to fit a linear regression model $$y = X \\beta + ɛ,$$\n",
        "such that $\\mathbb{E}[ɛ'ɛ]$ is minimized, where $\\mathbb{E}[ɛ_i]=0$ and\n",
        "$\\mathbb{V}[ɛ_i] = \\sigma^2$, for each $i=1,\\dotsc,n$.\n",
        "\n",
        "1.1 Derive the OLS solution $\\hat{\\beta}$ under the above objective. Show step by step.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbf{E} [\\varepsilon'\\varepsilon] &= \\mathbf{E}(y - X \\beta)^T (y - X \\beta) \\\\\n",
        "&= \\mathbf{E} (y^T y - 2 y^T X \\beta + \\beta^T X^T X \\beta) \\\\\n",
        "&= \\mathbf{E}[ (X \\beta + ϵ)^T (X \\beta + ϵ)] - \\beta^T X^T X \\beta \\\\\n",
        "&= \\beta^T X^T X \\beta - \\beta^T X^T X \\beta = 0\n",
        "\\end{align*}\n",
        "$$\n",
        "Thus for the expectation operation of the covariance of $ϵ$, we always would get 0, and since after taking the expectation operation, everything in this equation is fixed, hence nothing to optimize.Thus for deriving the OLS solution $\\hat{\\beta}$, we need to consider optimizing the $ϵ^{T} ϵ$, i.e.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\hat{\\beta} &= argmin_\\beta f(\\beta) \\\\\n",
        "f(\\beta) &=  y^T y - 2 y^T X \\beta + \\beta^T X^T X \\beta  \\\\\n",
        "\\frac{\\partial }{\\partial \\beta} f(\\beta) &= - 2 y^T X \\beta + \\beta^T X^T X \\beta \\\\\n",
        "\\hat{\\beta} &= (X^T X)^{-1} X^T y\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.2 Re-write the objective using a likelihood formulation assuming $ɛ_i \\sim N(0, \\sigma^2)$, for each $i=1,\\dotsc,n$.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "L(ɛ) &= \\prod^{n}_i \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\mathbf{e}^{\\frac{1}{2 \\sigma^2} ϵ_i^2}\\\\\n",
        "L(\\beta ) &= \\prod^{n}_i \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\mathbf{e}^{\\frac{1}{2 \\sigma^2} (y_i - X_i \\beta)^2}\\\\\n",
        "&= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\mathbf{e}^{\\frac{1}{2 \\sigma^2} \\sum^n_i (y_i - X_i \\beta)^2}\\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "where $ϵ_i = y_i - X_i \\beta$\n",
        "\n",
        "1.3 Derive the OLS solution $\\hat{\\beta}_{MLE}$ using the above objective. Show step by step.\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\frac{\\partial}{\\partial \\beta} \\log (L(ϵ)) &= - \\sum^n_i \\left( y_i X_i \\beta - \\beta^T X_i^T X_i \\beta \\right) ⇒\\\\\n",
        "\\hat{\\beta}_{MLE} &=  (\\sum^n_i X_i^T X_i)^{-1} (\\sum^n_i X_i^T y_i)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "1.4 Using [lineax](https://docs.kidger.site/lineax/), implement a solver for OLS."
      ],
      "metadata": {
        "id": "CtLu4_XIfaC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lineax as lx\n",
        "\n",
        "from jax import Array\n",
        "from jax.typing import ArrayLike\n",
        "\n",
        "\n",
        "def solve_ols(y: ArrayLike, X: ArrayLike) -> Array:\n",
        "  \"\"\"\n",
        "  Solves ordinary least squares using lineax.\n",
        "\n",
        "  y: ArrayLike of observations\n",
        "  X: ArrayLike of covariates\n",
        "\n",
        "  returns: $\\hat{\\beta}$ for OLS\n",
        "  \"\"\"\n",
        "\n",
        "  X_op = lx.MatrixLinearOperator(X)\n",
        "  solver = lx.NormalCG(rtol=1e-6, atol=1e-8)\n",
        "\n",
        "  return lx.linear_solve(X_op, y, solver=solver).value\n"
      ],
      "metadata": {
        "id": "f4f3welGhtMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Weighted least squares (i.e. WLS)\n",
        "WLS is an approach to fit a slightly more general linear model where, $$y = X \\beta + ɛ,$$ where $\\mathbb{E}[ɛ_i] = 0$ and $\\mathbb{V}[ɛ_i] = \\sigma^2_i$. We can model all variances jointly as $\\mathbb{V}[ɛ] = D$ where $D$ is a diagonal matrix such that $D_{ii} = \\sigma^2_i$.\n",
        "\n",
        "2.1 Write the WLS objective.\n",
        "\n",
        "Objective:\n",
        "$$\n",
        "argmin_\\beta \\ g(\\beta) \\\\\n",
        "$$\n",
        "$$\n",
        "g(\\beta) = (y - X\\beta)^T D^{-1}  (y - X \\beta)\n",
        "$$\n",
        "\n",
        "2.2. Derive the WLS solution $\\hat{\\beta}$ under the above objective. Show step by step.\n",
        "\n",
        "Here for convience, we first consider transform the original data. By definition, $V(ϵ) = D_{ii} = \\sigma^2_{ii}$, thus if we consider $\\tilde{ϵ} = D^{-\\frac{1}{2}} ϵ$, then by the weighted least squares assumption we have $\\tilde{y} = D^{-\\frac{1}{2}} y$, $\\tilde{X} = D^{-\\frac{1}{2}} X$, that is the original problem can be transformed to $\\tilde{y} = \\tilde{X} \\beta + \\tilde{ϵ}$, and $\\mathbf{E}(\\tilde{ϵ}) = 0$, $Var(\\tilde{ϵ}) = \\sigma^2$, which allows us to directly borrow the results from the OLS results, i.e.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\hat{\\beta} &= (\\tilde{X}^T \\tilde{X})^{-1} \\tilde{X}^T \\tilde{y} \\\\\n",
        "&= (X^T D^{-1} X)^{-1} X^T D^{-1} y\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "2.3. Re-write the objective using a likelihood formulation assuming $ɛ \\sim N(0, D)$.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\log L (\\beta) &\\propto \\sum^n_i \\frac{(y_i - X_i \\beta)^2}{\\sigma^2_i} \\propto \\sum^n_i \\frac{- 2 y_i X_i \\beta + \\beta^T X_i^T X_i \\beta}{\\sigma^2_i} \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "2.4 Derive the OLS solution $\\hat{\\beta}_{MLE}$ using the above objective. Show step by step.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial }{\\partial \\beta} \\log L (\\beta) &= \\frac{ -\\sum^n_i X_i^T y_i + \\sum^n_i  X_i^T X_i \\beta}{\\sigma^2_i} ⇒\\\\\n",
        "\\hat{\\beta} &= (\\sum^n_i X_i^T X_i / \\sigma_i^2)^{-1} (\\sum^n_i X_i^T y_i / \\sigma^2_i)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "2.5 Using [lineax](https://docs.kidger.site/lineax/), implement a solver for WLS."
      ],
      "metadata": {
        "id": "4sbheXNtiYy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lineax as lx\n",
        "\n",
        "from jax import Array\n",
        "from jax.typing import ArrayLike\n",
        "\n",
        "\n",
        "def solve_wls(y: ArrayLike, X: ArrayLike, D: ArrayLike) -> Array:\n",
        "  \"\"\"\n",
        "  Solves weighted least squares using lineax.\n",
        "\n",
        "  y: ArrayLike of observations\n",
        "  X: ArrayLike of covariates\n",
        "  D: ArrayLike of weights per observation\n",
        "\n",
        "  returns: $\\hat{\\beta}$ for WLS\n",
        "  \"\"\"\n",
        "\n",
        "  sqrtD = jnp.sqrt(D)\n",
        "  X_weighted = sqrtD[:, None] * X\n",
        "  y_weighted = sqrtD * y\n",
        "  X_op = lx.MatrixLinearOperator(X_weighted)\n",
        "\n",
        "  # Use normal CG solver for least squares (equivalent to WLS)\n",
        "  solver = lx.NormalCG(atol=1e-6, rtol=1e-6)\n",
        "  solution = lx.linear_solve(X_op, y_weighted, solver=solver)\n",
        "\n",
        "  return solution.value"
      ],
      "metadata": {
        "id": "HELjji9HjffX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. MLE for scalar Poisson observations\n",
        "Given $x_1, \\dotsc, x_n$, assume that $x_i \\sim \\text{Poi}(\\lambda)$ for $i=1,\\dotsc,n$ where $\\text{Poi}(\\lambda)$ is the Poisson distribution with rate $\\lambda$.\n",
        "\n",
        "3.1 Write a likelihood-based formulation of the objective.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\log L (\\lambda) &\\propto \\sum^n_i x_i \\log \\lambda -n\\lambda \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "3.2 Derive the MLE for the above objective. Show step by step.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial}{\\partial \\lambda} \\log L (\\lambda) &= \\frac{1}{\\lambda} \\sum^n_i x_i -n ⇒\\\\\n",
        "\\hat{\\lambda} &= \\bar{x} = \\frac{1}{n} \\sum^n_i x_i\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "3.3 Implement a function that simulates Poisson distributed data with rate $\\lambda$ using JAX.\n",
        "\n",
        "3.4 Implement a function that computes the MLE $\\hat{\\lambda}$ given observations $x_1, \\dotsc, x_n$."
      ],
      "metadata": {
        "id": "VXwMNL3fkDsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lineax as lx\n",
        "import jax.random as rdm\n",
        "\n",
        "from jax import Array\n",
        "from jax.typing import ArrayLike\n",
        "\n",
        "\n",
        "def simulate_poisson(key, rate: ArrayLike, n: int) -> Array:\n",
        "  \"\"\"\n",
        "  Simulates Poisson distributed data.\n",
        "\n",
        "  key: PRNGKey to generate\n",
        "  rate: rate specifying the Poisson distribution; can be either a scalar, or\n",
        "    ArrayLike (i.e. unique to each observation)\n",
        "  n: the number of samples to generate\n",
        "\n",
        "  returns: $x_i \\sim \\text{Poi}(\\lambda_i)$\n",
        "  \"\"\"\n",
        "  return rdm.poisson(key, rate, shape=(n,))\n",
        "\n",
        "\n",
        "def fit_poisson(x: ArrayLike) -> float:\n",
        "  \"\"\"\n",
        "  Fits Poisson distributed data.\n",
        "\n",
        "  x: ArrayLike observations\n",
        "\n",
        "  returns: estimate of $\\lambda$.\n",
        "  \"\"\"\n",
        "  return jnp.mean(x)"
      ],
      "metadata": {
        "id": "LY1UCqDBlF7l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}