{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yifdai/PM-520-repo/blob/main/HW/PM520_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1. Linear regression and normal equations"
      ],
      "metadata": {
        "id": "r94B4C5cpi8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Linear model simulation\n",
        "In class we defined a Python function that simulates $N$ $P\\times 1$ variables $X$ (i.e. an $N \\times P$ matrix $X$) and outcome $y$ as a linear function of $X$. Please include its definition here and use for problem 2."
      ],
      "metadata": {
        "id": "G7TX02bf92rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import jax.random as rdm\n",
        "\n",
        "def sim_linear_reg(key, N, P, r2=0.5):\n",
        "  key1, key2, key3 = rdm.split(key, n = 3)\n",
        "  design_mat = rdm.normal(key1, shape = (N, P))\n",
        "  sim_beta = rdm.normal(key2, shape = (N, ))\n",
        "  noise = rdm.normal(key3, shape = (N, ))\n",
        "  sim_y = design_mat @ sim_beta + noise\n",
        "  return sim_y"
      ],
      "metadata": {
        "id": "s1CpkFZh6Y7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Just-in time decorator and ordinary least squares\n",
        "Complete the definition of `ordinary_least_squares` below, that estimates the effect and its standard error. `@jit` wraps a function to perform just-in-time compilation, which boosts computational performance/speed.\n",
        "\n",
        "Compare the times of with and without JIT\n",
        "Hint: use [`block_until_ready()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.block_until_ready.html) to get correct timing estimates."
      ],
      "metadata": {
        "id": "Bq2qsE137hUK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3OWo_ixyh7Y"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "\n",
        "from jax import jit\n",
        "\n",
        "\n",
        "def ordinary_least_squares(X, y):\n",
        "  \"\"\"\n",
        "  computes the OLS solution to linear system y ~ X.\n",
        "  Returns a tuple of $\\hat{beta}$ and $\\text{se}(\\hat{beta})$.\n",
        "  \"\"\"\n",
        "  pass\n",
        "\n",
        "jit_ordinary_least_squares = jit(ordinary_least_squares)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. OLS derivation\n",
        "Assume that $y = X \\beta + \\epsilon$ where $y$ is $N \\times 1$ vector, $X$ is an $N \\times P$ matrix where $P < N$ and $\\epsilon$ is a random variable such that $\\mathbb{E}[\\epsilon_i] = 0$ and $\\mathbb{V}[\\epsilon_i] = \\sigma^2$ for all $i = 1 \\dots n$. Derive the OLS \"normal equations\"."
      ],
      "metadata": {
        "id": "pNGJ7Yij8gBt"
      }
    }
  ]
}