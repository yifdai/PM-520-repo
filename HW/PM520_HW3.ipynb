{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yifdai/PM-520-repo/blob/main/HW/PM520_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 3. Variational Inference"
      ],
      "metadata": {
        "id": "iVleC9nFW7sB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Evidence Lower Bound\n",
        "$\\newcommand{\\bX}{\\mathbf{X}}\\newcommand{\\by}{\\mathbf{y}}\\newcommand{\\bI}{\\mathbf{I}}$\n",
        "Recall from Lab 8, our example of variational inference for a Bayesian linear regression model. Namely,\n",
        "$$\\begin{align*}\n",
        "\\by | \\bX, \\beta &\\sim N(\\bX\\beta, \\bI_n \\sigma^2) \\\\\n",
        "\\beta &\\sim N(0, \\bI_p \\sigma^2_b).\n",
        "\\end{align*}$$\n",
        "\n",
        "We assumed a mean-field model that $Q$ factorizes as $$Q(\\beta) = \\prod_{j=1}^P Q_j(\\beta_j).$$\n",
        "\n",
        "### 1.1\n",
        "Consulting the results in Lab 8 on parameter definitions for each $Q_j$, please derive the *evidence lower bound* or ELBO for this model.\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "ELBO &= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\left[\\|\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\mu}\\|^2 + \\sigma^2\\sum_{j=1}^p \\|\\mathbf{X}_j\\|^2\\right] \\\\\n",
        "&\\quad -\\frac{p}{2}\\log(2\\pi\\sigma_b^2) - \\frac{1}{2\\sigma_b^2}\\left[\\|\\boldsymbol{\\mu}\\|^2 + p\\,\\sigma^2\\right] \\\\\n",
        "&\\quad -\\frac{p}{2}\\log(2\\pi \\sigma^2) - \\frac{p}{2}.\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "### 1.2\n",
        "Consult lab 8 for the implementation of a CAVI algorithm for the model above, but rather than evaluate the mean squared error (MSE), evaluate the ELBO. The ELBO should *increase* with each iteration, otherwise there is likely a bug."
      ],
      "metadata": {
        "id": "kVq8dl-TXbD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's code up the CAVI algorithm for bayesian linear regression\n",
        "\n",
        "import jax\n",
        "import jax.lax as lax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as rdm\n",
        "\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "MAX_ITER = 100\n",
        "\n",
        "N = 500\n",
        "P = 250\n",
        "sigma_sq = 0.8\n",
        "sigma_sq_b = 0.1\n",
        "\n",
        "seed = 0\n",
        "key = rdm.PRNGKey(seed)\n",
        "key, x_key, b_key, y_key = rdm.split(key, 4)\n",
        "\n",
        "X = rdm.normal(x_key, shape=(N, P))\n",
        "beta = jnp.sqrt(sigma_sq_b) * rdm.normal(b_key, shape=(P,))\n",
        "y = X @ beta + jnp.sqrt(sigma_sq) * rdm.normal(y_key, shape=(N,))\n",
        "\n",
        "post_means = jnp.zeros((P,))\n",
        "post_vars = jnp.ones((P,)) * sigma_sq_b\n",
        "\n",
        "def elbo(y, X, post_means, post_vars, sigma_sq):\n",
        "  Xsq = jnp.sum(X * X, axis=0)\n",
        "  pred = X @ post_means\n",
        "  term1 = -0.5 * N * jnp.log(sigma_sq)\n",
        "  term2 = jnp.sum(y ** 2) - 2 * y.T @ pred + jnp.sum(Xsq * post_vars) + pred @ pred\n",
        "\n",
        "  # expected log likelihood\n",
        "  exp_ll = term1 - term2 / (2 * sigma_sq)\n",
        "\n",
        "  # KL for each Q_j\n",
        "  kls = 0.5 * ((post_means ** 2 + post_vars) / sigma_sq_b - jnp.log(post_vars / sigma_sq_b) - 1.)\n",
        "\n",
        "  return exp_ll - jnp.sum(kls)\n",
        "\n",
        "def _inner(j, carry):\n",
        "  r, post_means, post_vars = carry\n",
        "\n",
        "  # update residual\n",
        "  Xj = X[:,j]\n",
        "  rj = r + Xj * post_means[j]\n",
        "\n",
        "  # update variational parameters for jth distribution\n",
        "  post_var_j = 1. / (jnp.sum(Xj ** 2) / sigma_sq + 1 / sigma_sq_b)\n",
        "  mu_j = rj @ X[:,j] * post_var_j / sigma_sq\n",
        "  post_vars = post_vars.at[j].set(post_var_j)\n",
        "  post_means = post_means.at[j].set(mu_j)\n",
        "\n",
        "  # remove the updated mean term from global residual\n",
        "  r = rj - Xj * mu_j\n",
        "  return r, post_means, post_vars\n",
        "\n",
        "\n",
        "last = -100000000\n",
        "for epoch in range(MAX_ITER):\n",
        "  r = y - X @ post_means\n",
        "  r, post_means, post_vars = lax.fori_loop(0, P, _inner, (r, post_means, post_vars))\n",
        "  value = elbo(y, X, post_means, post_vars, sigma_sq)\n",
        "  print(f\"ELBO[{epoch}] = {value}\")\n",
        "  diff = value - last\n",
        "  if diff < 0:\n",
        "    print(f\"something went wrong {diff}\")\n",
        "    break\n",
        "  if diff < 1e-3:\n",
        "    print(\"all done\")\n",
        "    break\n",
        "  last = value\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqYf3cgYE7n2",
        "outputId": "8f7d308c-6c46-467c-89f0-238c857b2715"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELBO[0] = -1509.9724636196884\n",
            "ELBO[1] = -868.4248784058789\n",
            "ELBO[2] = -764.261884492558\n",
            "ELBO[3] = -732.4241063447057\n",
            "ELBO[4] = -720.6285640613584\n",
            "ELBO[5] = -715.676069397937\n",
            "ELBO[6] = -713.4637318254369\n",
            "ELBO[7] = -712.4200619999491\n",
            "ELBO[8] = -711.9027873415175\n",
            "ELBO[9] = -711.6357059442223\n",
            "ELBO[10] = -711.4933224560815\n",
            "ELBO[11] = -711.4155484047511\n",
            "ELBO[12] = -711.3722767462873\n",
            "ELBO[13] = -711.3478586499151\n",
            "ELBO[14] = -711.3339252435566\n",
            "ELBO[15] = -711.3259023431558\n",
            "ELBO[16] = -711.3212475981998\n",
            "ELBO[17] = -711.3185292972977\n",
            "ELBO[18] = -711.316932642986\n",
            "ELBO[19] = -711.3159898860399\n",
            "all done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Bayesian Linear Regression Pt II\n",
        "Here we assume a slightly different linear model, which is given by, $$\\begin{align*}\n",
        "\\by | \\bX, \\beta &\\sim N(\\bX\\beta, \\bI_n \\sigma^2) \\\\\n",
        "\\beta_j &\\sim \\text{Laplace}(0, b).\n",
        "\\end{align*}$$\n",
        "\n",
        "We assumed a mean-field model that $Q$ factorizes as $$Q(\\beta) = \\prod_{j=1}^P Q_j(\\beta_j).$$ Rather than identify optimal $Q_j$ through CAVI, we will first assume $Q_j := \\text{Laplace}(\\mu_j, b_j)$. Next, to identify updates for each $\\mu_j, b_j$, we take the derivative of the ELBO with respect to each; however the gradient of the ELBO requires knowing $\\mu_j, b_j$, which causes challenges.\n",
        "\n",
        "### 2.1\n",
        "Re-write the ELBO as a deterministic transformation of $\\beta_j$ using location-scale rules (i.e. reparameterization trick)\n",
        "\n",
        "We apply the reparameterization trick here to first introduce two random variables that follows uniform distribution, $U_j, V_j \\sim \\text{Unif}(0, 1)$ independently. Then we have $\\log (\\frac{U_j}{V_j}) \\sim \\text{Laplace} (0, 1)$. Here since the laplace distribution belongs to the location-scale family, then we could represent $\\beta_j$ by a deterministic transformation: $\\beta_j (\\mu_j, b_j, U_j, V_j) = \\mu_j + b_j * \\log(\\frac{U_j}{V_j})$, where $\\beta_j \\sim \\text{Laplace} (\\mu_j, b_j)$\n",
        "\n",
        "\\begin{align*}\n",
        "f(\\beta_j | \\mu_j, b_j, U_j, V_j) &= \\frac{1}{2b_j} \\exp(- \\frac{|\\log(\\frac{U_j}{V_j}) - \\mu_j|}{b_j})\\\\\n",
        "\\frac{|\\beta_j - \\mu_j|}{b_j} &= |\\log(\\frac{U_j}{V_j})| \\\\\n",
        "\\end{align*}\n",
        "\n",
        "And to be more specific we re-define the prior as:\n",
        "\\begin{align*}\n",
        "\\beta \\sim \\text{Laplace} (0, b_0)\n",
        "\\end{align*}\n",
        "\n",
        "Then the ELBO is\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathrm{ELBO} = \\; \\mathbb{E}_{q(U, V)} \\Biggl\\{\\, &-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\Bigl\\| \\mathbf{y} - \\mathbf{X}\\left[\\mathbf{\\mu} + \\mathbf{b} \\circ \\left(\\log(\\mathbf{U}) - \\log(\\mathbf{V})\\right)\\right] \\Bigl\\|^2\\\\\n",
        "&- \\sum_{j=1}^{P}\\left[\\log(2b_0) + \\frac{\\Bigl|\\mu_j + b_j\\,\\log\\!\\left(\\frac{U_j}{V_j}\\right)\\Bigr|}{b_0}\\right]\\\\[1mm]\n",
        "&+\\sum_{j=1}^{P}\\left[\\log(2b_j) + \\log(U_j) - \\log(V_j)\\right]\n",
        "\\Biggr\\}\n",
        "\\end{align*}\n",
        "\n",
        "Based on transformation, we have\n",
        "$$\n",
        "U, V \\sim \\text{Unif}(0, 1)\\\\\n",
        "-\\log(U), -\\log(V) \\sim \\text{Exp}(1)\n",
        "$$\n",
        "so we have $\\mathbb{E}_{q(U)}(\\log(U)) = \\mathbb{E}_{q(V)}(\\log(V)) = -1$, the last term is then $\\sum_{j=1}^{P}\\log(2b_j)$.\n",
        "\n",
        "For the second term, we could transform it back to the laplace distribution and calculate the $\\mathbb{E}_q(\\beta_j) (|\\beta_j|)$\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_q(\\beta_j) (|\\beta_j|) &= \\int^{\\inf}_{\\mu_j} |\\beta_j| \\frac{1}{2b_j} \\exp(\\frac{\\beta_j - \\mu_j}{b_j})d\\beta_j + \\int^{0}_{-\\inf} |\\beta_j| \\frac{1}{2b_j} \\exp(\\frac{\\mu_j - \\beta_j}{b_j})d\\beta_j\n",
        "\\end{align*}\n",
        "\n",
        "Notice that it might be hard to directly work with it, thus we consider a transformation to center the laplace distribution by define\n",
        "$$\\gamma_j = \\beta_j - \\mu_j$$\\\n",
        "$$\\gamma_j \\sim \\text{Laplace}(0, b_j)$$\n",
        "Now that $\\gamma_j$ is centered around zero, then we have\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_q(|\\beta_j|)\n",
        " &= \\int_{-\\mu_j}^{\\infty} (\\gamma_j+\\mu_j) f(\\gamma_j) d\\gamma_j\n",
        "    + \\int_{-\\infty}^{-\\mu_j} -(\\gamma_j+\\mu_j) f(\\gamma_j) d\\gamma_j \\\\[1mm]\n",
        " &= \\frac{1}{2b_j} \\left[ \\int_{-\\mu_j}^{\\infty} (\\gamma_j+\\mu_j) \\exp\\left(-\\frac{\\gamma_j}{b_j}\\right) d\\gamma_j\n",
        "    + \\int_{-\\infty}^{-\\mu_j} -(\\gamma_j+\\mu_j) \\exp\\left(\\frac{\\gamma_j}{b_j}\\right) d\\gamma_j \\right] \\\\[1mm]\n",
        " &= |\\mu_j| + b_j \\exp\\left(-\\frac{|\\mu_j|}{b_j}\\right).\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Let $\\boldsymbol{\\epsilon} = \\log(\\mathbf{U}) - \\log(\\mathbf{V})$,  with $\\mathbb{E}(\\boldsymbol{\\epsilon}) = \\mathbf{0}$ and $\\mathrm{Var}(\\epsilon_j) = 2$. Then, we have $\\mathbf{z} = \\boldsymbol{\\mu} + \\mathbf{b} \\circ \\boldsymbol{\\epsilon}$, and $\\mathbf{X}\\mathbf{z} = \\mathbf{X}\\boldsymbol{\\mu} + \\mathbf{X}(\\mathbf{b} \\circ \\boldsymbol{\\epsilon})$.\n",
        "\n",
        "Thus we would have\n",
        "\\begin{align*}\n",
        "\\Bigl\\|\\mathbf{y} - \\mathbf{X}\\mathbf{z}\\Bigr\\|^2\n",
        "&= \\Bigl\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\mu} - \\mathbf{X}(\\mathbf{b}\\circ\\boldsymbol{\\epsilon})\\Bigr\\|^2 \\\\[1mm]\n",
        "&= \\Bigl\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\mu}\\Bigr\\|^2 - 2\\left(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\mu}\\right)^T\\mathbf{X}(\\mathbf{b}\\circ\\boldsymbol{\\epsilon}) \\\\\n",
        "&\\quad + \\Bigl\\|\\mathbf{X}(\\mathbf{b}\\circ\\boldsymbol{\\epsilon})\\Bigr\\|^2.\n",
        "\\end{align*}\n",
        "Taking expectation over $\\boldsymbol{\\epsilon}$ gives\n",
        "\\begin{align*}\n",
        "\\mathbb{E}\\Bigl[\\Bigl\\|\\mathbf{y} - \\mathbf{X}\\mathbf{z}\\Bigr\\|^2\\Bigr]\n",
        "&= \\Bigl\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\mu}\\Bigr\\|^2 - 2\\left(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\mu}\\right)^T\\mathbf{X}\\, \\mathbb{E}\\left[\\mathbf{b}\\circ\\boldsymbol{\\epsilon}\\right] \\\\\n",
        "&\\quad + \\mathbb{E}\\Bigl[\\Bigl\\|\\mathbf{X}(\\mathbf{b}\\circ\\boldsymbol{\\epsilon})\\Bigr\\|^2\\Bigr].\n",
        "\\end{align*}\n",
        "Since $\\mathbb{E}[\\boldsymbol{\\epsilon}] = \\mathbf{0}$, the middle term vanishes. Moreover,\n",
        "\\begin{align*}\n",
        "\\mathbb{E}\\Bigl[\\Bigl\\|\\mathbf{X}(\\mathbf{b}\\circ\\boldsymbol{\\epsilon})\\Bigr\\|^2\\Bigr]\n",
        "&= \\mathbb{E}\\left[\\sum_{j=1}^{P} \\left(b_j\\,\\epsilon_j\\right)^2 \\Bigl\\|\\mathbf{X}_{\\cdot j}\\Bigr\\|^2\\right] \\\\\n",
        "&= \\sum_{j=1}^{P} b_j^2\\,\\mathbb{E}(\\epsilon_j^2)\\,\\Bigl\\|\\mathbf{X}_{\\cdot j}\\Bigr\\|^2 \\\\\n",
        "&= 2\\sum_{j=1}^{P} b_j^2\\,\\Bigl\\|\\mathbf{X}_{\\cdot j}\\Bigr\\|^2.\n",
        "\\end{align*}\n",
        "Thus, the expected squared error is\n",
        "\\begin{align*}\n",
        "\\mathbb{E}\\Bigl[\\Bigl\\|\\mathbf{y} - \\mathbf{X}\\mathbf{z}\\Bigr\\|^2\\Bigr]\n",
        "&= \\Bigl\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\mu}\\Bigr\\|^2 + 2\\sum_{j=1}^{P} b_j^2\\,\\Bigl\\|\\mathbf{X}_{\\cdot j}\\Bigr\\|^2.\n",
        "\\end{align*}\n",
        "\n",
        "Thus, replacing the corresponding expectations, the full ELBO becomes\n",
        "\\begin{align*}\n",
        "\\mathrm{ELBO} =\\; & -\\frac{n}{2}\\log(2\\pi\\sigma^2)\n",
        "-\\frac{1}{2\\sigma^2}\\left\\{ \\Bigl\\| \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\mu} \\Bigr\\|^2 + 2\\sum_{j=1}^{P} b_j^2 \\Bigl\\| \\mathbf{X}_{\\cdot j} \\Bigr\\|^2 \\right\\}\\\\[1mm]\n",
        "& -\\sum_{j=1}^{P}\\left[\\log(2b_0) + \\frac{|\\mu_j| + b_j\\,\\exp\\!\\left(-\\frac{|\\mu_j|}{b_j}\\right)}{b_0}\\right]\n",
        "+ \\sum_{j=1}^{P}\\log(2b_j).\n",
        "\\end{align*}\n",
        "\n",
        "### 2.2\n",
        "Implement the above by performing stochastic VI to optimize the ELBO by sampling.\n",
        "\n"
      ],
      "metadata": {
        "id": "WehxWW76ZM_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "\n",
        "def compute_elbo(mu, sigma_sq, prior_var, y_batch, X_batch, n_total):\n",
        "    \"\"\"\n",
        "    Compute the ELBO estimate on a mini-batch.\n",
        "\n",
        "    ELBO = - n/2 * log(2πσ²)\n",
        "           - (1/(2σ²)) { ||y - Xμ||² + 2∑ b_j² ||X[:, j]||² }\n",
        "           - ∑ [ KL divergence between q(β_j) and the prior ]\n",
        "\n",
        "    Here we approximate the likelihood term using the mini-batch,\n",
        "    scaling its contribution by n_total / batch_size.\n",
        "    \"\"\"\n",
        "    batch_size = y_batch.shape[0]\n",
        "    eta = X_batch @ mu\n",
        "    mgf = jnp.exp(eta + 0.5 * ((X_batch * X_batch) @ sigma_sq))\n",
        "    exp_ll = y_batch @ eta - (n_total / batch_size) * jnp.sum(mgf)\n",
        "    kl = 0.5 * jnp.sum((mu ** 2 + sigma_sq) / prior_var - jnp.log(sigma_sq / prior_var) - 1)\n",
        "    return exp_ll - kl\n",
        "\n",
        "def compute_gradients(mu, sigma_sq, prior_var, y_batch, X_batch, n_total):\n",
        "    \"\"\"\n",
        "    Compute the gradients of the ELBO with respect to mu and sigma_sq on a mini-batch.\n",
        "    \"\"\"\n",
        "    batch_size = y_batch.shape[0]\n",
        "    eta = X_batch @ mu\n",
        "    mgf = jnp.exp(eta + 0.5 * ((X_batch * X_batch) @ sigma_sq))\n",
        "    grad_mu = X_batch.T @ (y_batch - (n_total / batch_size) * mgf) - mu / prior_var\n",
        "    grad_sigma_sq = -0.5 * (X_batch * X_batch).T @ ((n_total / batch_size) * mgf) - 0.5 / prior_var + 0.5 / sigma_sq\n",
        "    return grad_mu, grad_sigma_sq\n",
        "\n",
        "def stochastic_vi(y, X, prior_var=1e-3, step_size=1e-3, max_iter=2000, tol=1e-3, batch_size=20, seed=0):\n",
        "    \"\"\"\n",
        "    Perform SVI using mini-batch updates.\n",
        "    \"\"\"\n",
        "    key = random.PRNGKey(seed)\n",
        "    n, p = X.shape\n",
        "    mu = jnp.zeros(p)\n",
        "    sigma_sq = jnp.ones(p) * prior_var\n",
        "\n",
        "    elbo_val = compute_elbo(mu, sigma_sq, prior_var, y, X, n)\n",
        "    print(\"Initial ELBO (full data) =\", elbo_val)\n",
        "\n",
        "    for epoch in range(max_iter):\n",
        "        key, subkey = random.split(key)\n",
        "        indices = random.choice(subkey, n, shape=(batch_size,), replace=False)\n",
        "        X_batch = X[indices]\n",
        "        y_batch = y[indices]\n",
        "\n",
        "        grad_mu, grad_sigma_sq = compute_gradients(mu, sigma_sq, prior_var, y_batch, X_batch, n)\n",
        "        mu = mu + step_size * grad_mu\n",
        "        sigma_sq = sigma_sq + step_size * grad_sigma_sq\n",
        "\n",
        "        new_elbo = compute_elbo(mu, sigma_sq, prior_var, y, X, n)\n",
        "        delta = jnp.abs(new_elbo - elbo_val)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Iteration {epoch}, ELBO (full data) = {new_elbo}, Δ = {delta}\")\n",
        "        if delta < tol:\n",
        "            break\n",
        "        elbo_val = new_elbo\n",
        "\n",
        "    return mu, sigma_sq, elbo_val\n",
        "\n",
        "def simulate_poisson_data(n=200, p=10, seed=42):\n",
        "    \"\"\"\n",
        "    Simulate data for a Poisson regression problem.\n",
        "\n",
        "    y ~ Poisson(exp(X @ true_coeff))\n",
        "    \"\"\"\n",
        "    key = random.PRNGKey(seed)\n",
        "    X = random.normal(key, shape=(n, p))\n",
        "    true_coeff = jnp.linspace(0.5, 2.0, p)\n",
        "    eta = X @ true_coeff\n",
        "    y = random.poisson(key, lam=jnp.exp(eta))\n",
        "    return y, X, true_coeff\n",
        "\n",
        "\n",
        "n = 200\n",
        "p = 10\n",
        "y, X, true_coeff = simulate_poisson_data(n=n, p=p, seed=0)\n",
        "print(\"True coefficients:\", true_coeff)\n",
        "\n",
        "mu_opt, sigma_sq_opt, final_elbo = stochastic_vi(\n",
        "    y, X, prior_var=1e-3, step_size=1e-3, max_iter=2000, tol=1e-3, batch_size=20, seed=0\n",
        ")\n",
        "\n",
        "print(\"\\nOptimized variational parameters:\")\n",
        "print(\"mu =\", mu_opt)\n",
        "print(\"sigma_sq =\", sigma_sq_opt)\n",
        "print(\"Final ELBO =\", final_elbo)\n"
      ],
      "metadata": {
        "id": "A3fGpXPefj-l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}